{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pds",
   "display_name": "pds"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Histopathologic Cancer Detection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 1. Installing libraries, downloading the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(101)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "source": [
    "We are not allowed to share the dataset. Download the dataset from Kaggle by pip installing kaggle, and running `kaggle competitions download -c histopathologic-cancer-detection` in your terminal. Then, extract files from the histopathologic-cancer-detection.zip folder into a new folder called 'data'."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 96\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "SAMPLE_SIZE = 80000 # the number of images we use from each of the two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See image count in each folder\n",
    "print(len(os.listdir('data/train')))\n",
    "print(len(os.listdir('data/test')))"
   ]
  },
  {
   "source": [
    "# 2. Data Pre-processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now that the data has been properly loaded and set-up, we must now pre-process our data: in our case, we mainnly subset the data, augment the images, and split it into train and test.\n",
    "\n",
    "Note: The data takes hours to download in its full size, leaving a high possibility of crashing the kernel, not to mention the time required to train the model. Thus, we slightly modified someone else's preprocessing instructions throughout this entire stage to get a smaller subset of the data. This will make it faster and easier for us to run and train our model.\n",
    "\n",
    "https://www.kaggle.com/vbookshelf/cnn-how-to-use-160-000-images-without-crashing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Removing anomalous images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataframe containing all images\n",
    "data = pd.read_csv('data/train_labels.csv')\n",
    "\n",
    "# Removing this image because it caused a training error previously\n",
    "data[data['id'] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2']\n",
    "\n",
    "# Removing this image because it's black\n",
    "data[data['id'] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe']\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "source": [
    "## Augmenting image data\n",
    "\n",
    "Justification for augmentation\n",
    "\n",
    "This Kaggle challenge is a Machine Learning challenge. Machine learning however, requires plenty and diverse training data to accurately predict future data points without overfitting or underfitting. We have a lot of data points already, but also want to diversify them to prevent overfitting to the original data that may be too similar to each other already. We decided to augment the images in order to increase the diversity of images as inspired by this github repo below.\n",
    "\n",
    "https://github.com/aleju/imgaug"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for augmenting data\n",
    "from skimage.transform import rotate, AffineTransform\n",
    "import cv2\n",
    "from skimage.util import random_noise\n",
    "import random\n",
    "import os\n",
    "from skimage import io\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "ORIGINAL_SIZE = IMAGE_SIZE      # original size of the images - do not change\n",
    "\n",
    "# AUGMENTATION VARIABLES\n",
    "CROP_SIZE = 90          # final size after crop\n",
    "RANDOM_ROTATION = 3    # range (0-180), 180 allows all rotation variations, 0=no change\n",
    "RANDOM_SHIFT = 2        # center crop shift in x and y axes, 0=no change. This cannot be more than (ORIGINAL_SIZE - CROP_SIZE)//2 \n",
    "RANDOM_BRIGHTNESS = 7  # range (0-100), 0=no change\n",
    "RANDOM_CONTRAST = 5    # range (0-100), 0=no change\n",
    "RANDOM_90_DEG_TURN = 1  # 0 or 1= random turn to left or right\n",
    "\n",
    "def readCroppedImage(path, augmentations = True):\n",
    "    '''\n",
    "    This is a custom function to convert an input image, augment it through\n",
    "    random rotation, random x or y shift, random cropping, random flipping, \n",
    "    random changes in brightness and contrast, and returning it as an rgb tensor.\n",
    "    '''\n",
    "    # augmentations parameter is included for counting statistics from images, where we don't want augmentations\n",
    "    \n",
    "    # OpenCV reads the image in bgr format by default\n",
    "    bgr_img = cv2.imread(path)\n",
    "    # We flip it to rgb for visualization purposes\n",
    "    b,g,r = cv2.split(bgr_img)\n",
    "    rgb_img = cv2.merge([r,g,b])\n",
    "    \n",
    "    if(not augmentations):\n",
    "        return rgb_img / 255\n",
    "    \n",
    "    #random rotation\n",
    "    rotation = random.randint(-RANDOM_ROTATION,RANDOM_ROTATION)\n",
    "    if(RANDOM_90_DEG_TURN == 1):\n",
    "        rotation += random.randint(-1,1) * 90\n",
    "    M = cv2.getRotationMatrix2D((48,48),rotation,1)   # the center point is the rotation anchor\n",
    "    rgb_img = cv2.warpAffine(rgb_img,M,(96,96))\n",
    "    \n",
    "    #random x,y-shift\n",
    "    x = random.randint(-RANDOM_SHIFT, RANDOM_SHIFT)\n",
    "    y = random.randint(-RANDOM_SHIFT, RANDOM_SHIFT)\n",
    "    \n",
    "    # crop to center and normalize to 0-1 range\n",
    "    start_crop = (ORIGINAL_SIZE - CROP_SIZE) // 2\n",
    "    end_crop = start_crop + CROP_SIZE\n",
    "    rgb_img = rgb_img[(start_crop + x):(end_crop + x), (start_crop + y):(end_crop + y)] / 255\n",
    "    \n",
    "    # Random flip\n",
    "    flip_hor = bool(random.getrandbits(1))\n",
    "    flip_ver = bool(random.getrandbits(1))\n",
    "    if(flip_hor):\n",
    "        rgb_img = rgb_img[:, ::-1]\n",
    "    if(flip_ver):\n",
    "        rgb_img = rgb_img[::-1, :]\n",
    "        \n",
    "    # Random brightness\n",
    "    br = random.randint(-RANDOM_BRIGHTNESS, RANDOM_BRIGHTNESS) / 100.\n",
    "    rgb_img = rgb_img + br\n",
    "    \n",
    "    # Random contrast\n",
    "    cr = 1.0 + random.randint(-RANDOM_CONTRAST, RANDOM_CONTRAST) / 100.\n",
    "    rgb_img = rgb_img * cr\n",
    "    \n",
    "    # clip values to 0-1 range\n",
    "    rgb_img = np.clip(rgb_img, 0, 1.0)\n",
    "    \n",
    "    return img_as_ubyte(rgb_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augment test images randomly\n",
    "\n",
    "images_path=\"data/train\" #path to original images\n",
    "augmented_path=\"data/train\" # path to store augmented images\n",
    "images=[] # to store paths of images from folder\n",
    "\n",
    "for im in os.listdir(images_path):  # read image name from folder and append its path into \"images\" array     \n",
    "    images.append(os.path.join(images_path,im))\n",
    "\n",
    "images_to_generate=5000 #you can change this value according to your requirement\n",
    "\n",
    "for i in range(images_to_generate):    \n",
    "    image=random.choice(images)\n",
    "    id = image[11:-4]\n",
    "    label = data[data['id'] == id].iloc[0]['label']\n",
    "    data = data.append({\"id\":'augmented_'+id,'label':label},ignore_index=True)\n",
    "    transformed_image= readCroppedImage(image)\n",
    "    new_image_path= \"train/augmented_%s.tif\" %(id)\n",
    "    cv2.imwrite(new_image_path, transformed_image) # save transformed image to path\n",
    "\n",
    "#Save new label file which has the augmented images\n",
    "data.to_csv('data/new_train_labels.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new csv that now includes the augmented images \n",
    "df_data = pd.read_csv('data/new_train_labels.csv')\n",
    "\n",
    "# Check the class distribution\n",
    "df_data['label'].value_counts()"
   ]
  },
  {
   "source": [
    "## Balance the target distribution\n",
    "\n",
    "As decided earlier with the variable SAMPLE_SIZE, we will subset our original data into 160000 images half labelled 0, the other labelled 1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a random sample of class 0 with size equal to num samples in class 1\n",
    "df_0 = df_data[df_data['label'] == 0].sample(SAMPLE_SIZE, random_state = 101)\n",
    "# filter out class 1\n",
    "df_1 = df_data[df_data['label'] == 1].sample(SAMPLE_SIZE, random_state = 101)\n",
    "\n",
    "# concat the dataframes\n",
    "df_data = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n",
    "# shuffle\n",
    "df_data = shuffle(df_data)\n",
    "\n",
    "df_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "\n",
    "# stratify=y creates a balanced validation set.\n",
    "y = df_data['label']\n",
    "\n",
    "df_train, df_val = train_test_split(df_data, test_size=0.10, random_state=101, stratify=y)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the training set counts\n",
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the validation set counts\n",
    "df_val['label'].value_counts()"
   ]
  },
  {
   "source": [
    "## Directory structure for image generators"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Creating the folders"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new directory\n",
    "base_dir = 'data/base_dir'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n",
    "\n",
    "# now we create 2 folders inside 'base_dir':\n",
    "\n",
    "# train_dir\n",
    "    # a_no_tumor_tissue\n",
    "    # b_has_tumor_tissue\n",
    "\n",
    "# val_dir\n",
    "    # a_no_tumor_tissue\n",
    "    # b_has_tumor_tissue\n",
    "\n",
    "\n",
    "# create a path to 'base_dir' to which we will join the names of the new folders\n",
    "# train_dir\n",
    "train_dir = os.path.join(base_dir, 'train_dir')\n",
    "os.mkdir(train_dir)\n",
    "\n",
    "# val_dir\n",
    "val_dir = os.path.join(base_dir, 'val_dir')\n",
    "os.mkdir(val_dir)\n",
    "\n",
    "# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n",
    "# Inside each folder we create seperate folders for each class\n",
    "\n",
    "# create new folders inside train_dir\n",
    "no_tumor_tissue = os.path.join(train_dir, 'a_no_tumor_tissue')\n",
    "os.mkdir(no_tumor_tissue)\n",
    "has_tumor_tissue = os.path.join(train_dir, 'b_has_tumor_tissue')\n",
    "os.mkdir(has_tumor_tissue)\n",
    "\n",
    "# create new folders inside val_dir\n",
    "no_tumor_tissue = os.path.join(val_dir, 'a_no_tumor_tissue')\n",
    "os.mkdir(no_tumor_tissue)\n",
    "has_tumor_tissue = os.path.join(val_dir, 'b_has_tumor_tissue')\n",
    "os.mkdir(has_tumor_tissue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the folders have been created\n",
    "os.listdir('data/base_dir/train_dir')"
   ]
  },
  {
   "source": [
    "## Splitting images into folders"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the id as the index in df_data\n",
    "df_data.set_index('id', inplace=True)\n",
    "\n",
    "# Get a list of train and val images\n",
    "train_list = list(df_train['id'])\n",
    "val_list = list(df_val['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the train images\n",
    "count = 0\n",
    "\n",
    "for image in train_list:\n",
    "    count += 1\n",
    "    # the id in the csv file does not have the .tif extension therefore we add it here\n",
    "    fname = image + '.tif'\n",
    "    # get the label for a certain image\n",
    "    target = df_data.loc[image,'label']\n",
    "\n",
    "    # these must match the folder names\n",
    "    if target.any() == 0:\n",
    "        label = 'a_no_tumor_tissue'\n",
    "    if target.any() == 1:\n",
    "        label = 'b_has_tumor_tissue'\n",
    "    \n",
    "    # source path to image\n",
    "    src = os.path.join('data/train', fname)\n",
    "    # destination path to image\n",
    "    dst = os.path.join(train_dir, label, fname)\n",
    "    # copy the image from the source to the destination\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "\n",
    "# Transfer the val images\n",
    "\n",
    "for image in val_list:\n",
    "    \n",
    "    # the id in the csv file does not have the .tif extension therefore we add it here\n",
    "    fname = image + '.tif'\n",
    "    # get the label for a certain image\n",
    "    target = df_data.loc[image,'label']\n",
    "    \n",
    "    # these must match the folder names\n",
    "    if target.any() == 0:\n",
    "        label = 'a_no_tumor_tissue'\n",
    "    if target.any() == 1:\n",
    "        label = 'b_has_tumor_tissue'\n",
    "    \n",
    "\n",
    "    # source path to image\n",
    "    src = os.path.join('data/train', fname)\n",
    "    # destination path to image\n",
    "    dst = os.path.join(val_dir, label, fname)\n",
    "    # copy the image from the source to the destination\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many train images we have in each folder\n",
    "print(len(os.listdir('data/base_dir/train_dir/a_no_tumor_tissue')))\n",
    "print(len(os.listdir('data/base_dir/train_dir/b_has_tumor_tissue')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many val images we have in each folder\n",
    "print(len(os.listdir('data/base_dir/val_dir/a_no_tumor_tissue')))\n",
    "print(len(os.listdir('data/base_dir/val_dir/b_has_tumor_tissue')))\n"
   ]
  },
  {
   "source": [
    "## Setting up the generators"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/base_dir/train_dir'\n",
    "valid_path = 'data/base_dir/val_dir'\n",
    "\n",
    "num_train_samples = len(df_train)\n",
    "num_val_samples = len(df_val)\n",
    "train_batch_size = 10\n",
    "val_batch_size = 10\n",
    "\n",
    "train_steps = np.ceil(num_train_samples / train_batch_size)\n",
    "val_steps = np.ceil(num_val_samples / val_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(train_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=train_batch_size,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "val_gen = datagen.flow_from_directory(valid_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=val_batch_size,\n",
    "                                        class_mode='categorical')"
   ]
  },
  {
   "source": [
    "# 4. Model creation & assessment\n",
    "\n",
    "This is the stage where we create a model that trains on the data. In our case, we chose to manually create a neural network model implemented with the keras library. We created a feedforward neural network with 24 layers all using the relu activation function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Logistic Regression Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Manually built CNN Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "kernel_size = (3,3)\n",
    "pool_size= (2,2)\n",
    "first_filters = 32\n",
    "second_filters = 64\n",
    "third_filters = 128\n",
    "\n",
    "dropout_conv = 0.3\n",
    "dropout_dense = 0.3\n",
    "\n",
    "# Neural network creation and layer adding\n",
    "model = Sequential()\n",
    "model.add(Conv2D(first_filters, kernel_size, activation = 'relu', input_shape = (96, 96, 3)))\n",
    "model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
    "model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = pool_size)) \n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
    "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
    "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size = pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
    "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
    "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size = pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(dropout_dense))\n",
    "model.add(Dense(2, activation = \"softmax\"))\n",
    "\n",
    "# Key details for each layer\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(Adam(lr=0.0001), loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Get the labels that are associated with each index\n",
    "print(val_gen.class_indices)"
   ]
  },
  {
   "source": [
    "### Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train through 20 epochs in hopes to improve accuracy\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, \n",
    "                             save_best_only=True, mode='max')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=2, \n",
    "                                   verbose=0, mode='max', min_lr=0.00001)\n",
    "                                                      \n",
    "callbacks_list = [checkpoint, reduce_lr]\n",
    "\n",
    "history = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=20, verbose=0,\n",
    "                   callbacks=callbacks_list)\n",
    "\n",
    "# Train through 20 epochs in hopes to improve accuracy\n",
    "\n",
    "filepath = \"model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, \n",
    "                             save_best_only=True, mode='max')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=2, \n",
    "                                   verbose=0, mode='max', min_lr=0.00001)\n",
    "                                                      \n",
    "callbacks_list = [checkpoint, reduce_lr]\n",
    "\n",
    "history = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=20, verbose=0,\n",
    "                   callbacks=callbacks_list)\n",
    "\n",
    "model.save(\"cnn_model.h5\")\n",
    "\n",
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df= pd.DataFrame(history.history) \n",
    "# or save to csv: \n",
    "hist_csv_file = 'cnn_model_history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "source": [
    "## VGG16 Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "# VGG model without the last classifier layers (include_top = False)\n",
    "vgg16_model = VGG16(include_top = False,\n",
    "                    input_shape = (96,96,3),\n",
    "                    #weights='../input/VGG16weights/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "                    weights = 'imagenet')\n",
    "    \n",
    "# Freeze the layers \n",
    "for layer in vgg16_model.layers[:-12]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Check the trainable status of the individual layers\n",
    "for layer in vgg16_model.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Dropout\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(vgg16_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimizers.SGD(lr=0.00001, momentum=0.95),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the model using the train set\n",
    "\n",
    "checkpoint_2 = ModelCheckpoint(filepath_2, monitor='val_accuracy', verbose=1, \n",
    "                             save_best_only=True, mode='max')\n",
    "\n",
    "reduce_lr_2 = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=2, \n",
    "                                   verbose=0, mode='max', min_lr=0.00001)\n",
    "                              \n",
    "                              \n",
    "callbacks_list_2 = [checkpoint_2, reduce_lr_2]\n",
    "\n",
    "history_2 =  model.fit_generator(train_gen, steps_per_epoch=train_steps, \n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=20, verbose=0,\n",
    "                   callbacks=callbacks_list_2)\n",
    "\n",
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df= pd.DataFrame(history_2.history) \n",
    "\n",
    "# or save to csv: \n",
    "hist_csv_file = 'vgg16_model_history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "source": [
    "# 4. Evaluating our model\n",
    "\n",
    "In this section, we will load our CNN model (model.h5) and use it to classify images in the test set. We will then plot the confusion matrix, AUC-ROC, and submit our results to Kaggle and check out our ranking. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Kaggle submission\n",
    "Preparing the generators, producing the classifications and uploading it to Kaggle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['a_no_tumor_tissue', 'b_has_tumor_tissue']"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "# Create directory structure for generators\n",
    "\n",
    "os.mkdir('data/base_dir')\n",
    "test_path = 'data/base_dir/test_dir'\n",
    "os.mkdir(test_path) # Make a directory for the test images\n",
    "os.mkdir(os.path.join(test_path,'a_no_tumor_tissue')) # label 0\n",
    "os.mkdir(os.path.join(test_path,'b_has_tumor_tissue')) # label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "##### FIRST MODEL: Manually built model\n",
    "\n",
    "model_cnn = load_model(\"model.h5\")\n",
    "\n",
    "#Confusion Matrix and Classification Report\n",
    "\n",
    "Y_pred = model_cnn.predict(x = val_gen, batch_size = num_val_samples//val_batch_size)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "cm = confusion_matrix(val_gen.classes, y_pred)"
   ]
  },
  {
   "source": [
    "# 5. Interpreting our model\n",
    "\n",
    "We now attempt to interpret our convolutional neural network model, to understand what features in an image are important to classifying it as showing a breast tumor or not. \n",
    "\n",
    "LIME (locally interpretable model-agnostic explainer) will be used, as is conventional for image classification."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}