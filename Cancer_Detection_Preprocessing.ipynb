{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "[Draft] Cancer Detection Preprocessing.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdPnbjZe5i2w"
      },
      "source": [
        "# Histopathologic Cancer Detection\n",
        "Authors: Abdul Qadir, Asmaa Aly, Wei-Ting Yap, Nathan Torento\n",
        "\n",
        "Course: Practical Data Science at Minerva Schools at KGI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzzAzNOp5l3f"
      },
      "source": [
        "# Introduction\n",
        "This paper's dataset is taken from the Kaggle competition on Histopathologic Cancer Detection. It uses the PatchCamelyon (PCam) dataset, around 300k fixed-size histopathology (the study of tissue disease)\n",
        "colored scans of lymph nodes all around the body. \n",
        "\n",
        "The specific challenge in the original dataset and competition is to train a model that can most accurately detect metastatic cancer.\n",
        "\n",
        "The overall .zip file contains pictures and train-test csv files. The .csv files contains only two columns: id, and label, where the id contains the unique id or name of the picture, and the label determines whether the picture is indeed indicative of metastatic cancer.\n",
        "\n",
        "This paper is created by Abdul Qadir, Asmaa Alaa Aly, Wei-Ting Yap, and Nathan Torento. For their and the reader's convenience, code and text are all written in this Google Colab notebook. It consists of four parts that they've split amongst themselves.\n",
        "\n",
        "1. Data preparation and exploration\n",
        "\n",
        "2. Data pre-processing\n",
        "\n",
        "3. Model creation assessment\n",
        "\n",
        "4. Presentation of findings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK4EGsKl5w2H"
      },
      "source": [
        "# 1. Data preparation and exploration\n",
        "\n",
        "### Downloading Dataset (Guide)\n",
        "\n",
        "We are not allowed to share the dataset ourselves. Simply follow the instructions below, and at some point, you will be gain permission to download the data from the Kaggle website yourself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "mf0Lnecg4E57"
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(101)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import itertools\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AahTYY3Y5VfS",
        "outputId": "a4254643-56bf-4819-b508-c6801d23832a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Install a kaggle package to download the dataset\n",
        "! pip install -q kaggle\n",
        "! pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kaggle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/14/9db40d8d6230655e76fa12166006f952da4697c003610022683c514cf15f/kaggle-1.5.8.tar.gz (59kB)\n",
            "\r\u001b[K     |█████▌                          | 10kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 20kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 30kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 40kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.8-cp36-none-any.whl size=73275 sha256=0d1a7b3f6e51336b1c692678919ae6f5af59f5c755059326981aee1635c54ea8\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/a7/09/68dc83c7c14fdbdf5d3f2b2da5b87e587bfc1e85df69b1130c\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Found existing installation: kaggle 1.5.8\n",
            "    Uninstalling kaggle-1.5.8:\n",
            "      Successfully uninstalled kaggle-1.5.8\n",
            "Successfully installed kaggle-1.5.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHQkyvQh57z8"
      },
      "source": [
        "Follow the steps in the following link. \n",
        "\n",
        "You should have a **kaggle.json** file at the end of it.\n",
        "\n",
        "https://www.kaggle.com/general/74235"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4sSwD1c5WLG",
        "outputId": "2be02150-a8c4-40ad-eea3-2156deb0c385",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Run this cell, then upload your \"kaggle.json\" file when prompted.\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1d8ddc48-ebcc-40ec-95b3-f61d27c33ac4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1d8ddc48-ebcc-40ec-95b3-f61d27c33ac4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"ntorento\",\"key\":\"323b6b306242b2dbea4604c48c54ee8d\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5FZzgWC6B7e"
      },
      "source": [
        "IMAGE_SIZE = 96\n",
        "IMAGE_CHANNELS = 3\n",
        "\n",
        "SAMPLE_SIZE = 80000 # the number of images we use from each of the two classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGxWXxbm5SRS",
        "outputId": "71f706da-ab70-489b-e20e-3f25a25c51f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Below is code to gain permission to download the dataset\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "#! kaggle datasets list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5lfO3ky5MT1",
        "outputId": "025d4f63-58e6-4dbc-8ac2-63cf0a0397ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Download the desired dataset (in the default zip format)\n",
        "\n",
        "! kaggle competitions download -c histopathologic-cancer-detection"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading histopathologic-cancer-detection.zip to /content\n",
            "100% 6.30G/6.31G [01:35<00:00, 80.1MB/s]\n",
            "100% 6.31G/6.31G [01:35<00:00, 70.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0fRdBwO59KD"
      },
      "source": [
        "# Unzip and load the dataset onto your colab runtime\n",
        "import zipfile\n",
        "zip = zipfile.ZipFile('histopathologic-cancer-detection.zip')\n",
        "zip.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "54461212efed65ac377369a468c80e7d708010f4",
        "id": "TJZ3daFy4E6F",
        "outputId": "4a6e0ce6-34d5-42ea-995b-377bf6da689f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# See image count in each folder?\n",
        "print(len(os.listdir('../content/train')))\n",
        "print(len(os.listdir('../content/test')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "220025\n",
            "57458\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnRMacOc_4eu"
      },
      "source": [
        "# 2. Data pre-processing\n",
        "\n",
        "Now that the data has been properly loaded and set-up, we must now pre-process our data: in our case, we mainnly subset the data, augment the images, and split it into train and test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkgHyXzG6drs"
      },
      "source": [
        "Note: The data takes hours to download in its full size, leaving a high possibility of crashing the kernel, not to mention the time required to train the model. Thus, we slightly modified someone else's preprocessing instructions throughout this entire stage to get a smaller subset of the data. This will make it faster and easier for us to run and train our model.\n",
        "\n",
        "https://www.kaggle.com/vbookshelf/cnn-how-to-use-160-000-images-without-crashing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "e9c9f40ffab35044641b0dc7d9b18609af1aa25e",
        "id": "3C9YWx4s4E6I",
        "outputId": "42018cea-7f8a-445f-983b-b3212187d643",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create a Dataframe containing all images\n",
        "data = pd.read_csv('../content/train_labels.csv')\n",
        "\n",
        "# Removing this image because it caused a training error previously\n",
        "data[data['id'] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2']\n",
        "\n",
        "# Removing this image because it's black\n",
        "data[data['id'] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe']\n",
        "\n",
        "print(data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(220025, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFrsX06d8757"
      },
      "source": [
        "Justification for augmentation\n",
        "\n",
        "This Kaggle challenge is a Machine Learning challenge. Machine learning however, requires plenty and diverse training data to accurately predict future data points without overfitting or underfitting. We have a lot of data points already, but also want to diversity them to prevent overfitting to the original data that may be too similar to each other already. We decided to augment the images in order to increase the diversity of images as inspired by this github repo below.\n",
        "\n",
        "https://github.com/aleju/imgaug"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y65QQlo1bClI"
      },
      "source": [
        "#Function for augmenting data\n",
        "from skimage.transform import rotate, AffineTransform\n",
        "import cv2\n",
        "from skimage.util import random_noise\n",
        "import random\n",
        "import os\n",
        "from skimage import io\n",
        "from skimage import img_as_ubyte\n",
        "\n",
        "ORIGINAL_SIZE = 96      # original size of the images - do not change\n",
        "\n",
        "# AUGMENTATION VARIABLES\n",
        "CROP_SIZE = 90          # final size after crop\n",
        "RANDOM_ROTATION = 3    # range (0-180), 180 allows all rotation variations, 0=no change\n",
        "RANDOM_SHIFT = 2        # center crop shift in x and y axes, 0=no change. This cannot be more than (ORIGINAL_SIZE - CROP_SIZE)//2 \n",
        "RANDOM_BRIGHTNESS = 7  # range (0-100), 0=no change\n",
        "RANDOM_CONTRAST = 5    # range (0-100), 0=no change\n",
        "RANDOM_90_DEG_TURN = 1  # 0 or 1= random turn to left or right\n",
        "\n",
        "def readCroppedImage(path, augmentations = True):\n",
        "    '''\n",
        "    This is a custom function to convert an input image, augment it through\n",
        "    random rotation, random x or y shift, random cropping, random flipping, \n",
        "    random changes in brightness and contrast, and returning it as an rgb tensor.\n",
        "    '''\n",
        "    # augmentations parameter is included for counting statistics from images, where we don't want augmentations\n",
        "    \n",
        "    # OpenCV reads the image in bgr format by default\n",
        "    bgr_img = cv2.imread(path)\n",
        "    # We flip it to rgb for visualization purposes\n",
        "    b,g,r = cv2.split(bgr_img)\n",
        "    rgb_img = cv2.merge([r,g,b])\n",
        "    \n",
        "    if(not augmentations):\n",
        "        return rgb_img / 255\n",
        "    \n",
        "    #random rotation\n",
        "    rotation = random.randint(-RANDOM_ROTATION,RANDOM_ROTATION)\n",
        "    if(RANDOM_90_DEG_TURN == 1):\n",
        "        rotation += random.randint(-1,1) * 90\n",
        "    M = cv2.getRotationMatrix2D((48,48),rotation,1)   # the center point is the rotation anchor\n",
        "    rgb_img = cv2.warpAffine(rgb_img,M,(96,96))\n",
        "    \n",
        "    #random x,y-shift\n",
        "    x = random.randint(-RANDOM_SHIFT, RANDOM_SHIFT)\n",
        "    y = random.randint(-RANDOM_SHIFT, RANDOM_SHIFT)\n",
        "    \n",
        "    # crop to center and normalize to 0-1 range\n",
        "    start_crop = (ORIGINAL_SIZE - CROP_SIZE) // 2\n",
        "    end_crop = start_crop + CROP_SIZE\n",
        "    rgb_img = rgb_img[(start_crop + x):(end_crop + x), (start_crop + y):(end_crop + y)] / 255\n",
        "    \n",
        "    # Random flip\n",
        "    flip_hor = bool(random.getrandbits(1))\n",
        "    flip_ver = bool(random.getrandbits(1))\n",
        "    if(flip_hor):\n",
        "        rgb_img = rgb_img[:, ::-1]\n",
        "    if(flip_ver):\n",
        "        rgb_img = rgb_img[::-1, :]\n",
        "        \n",
        "    # Random brightness\n",
        "    br = random.randint(-RANDOM_BRIGHTNESS, RANDOM_BRIGHTNESS) / 100.\n",
        "    rgb_img = rgb_img + br\n",
        "    \n",
        "    # Random contrast\n",
        "    cr = 1.0 + random.randint(-RANDOM_CONTRAST, RANDOM_CONTRAST) / 100.\n",
        "    rgb_img = rgb_img * cr\n",
        "    \n",
        "    # clip values to 0-1 range\n",
        "    rgb_img = np.clip(rgb_img, 0, 1.0)\n",
        "    \n",
        "    return img_as_ubyte(rgb_img)\n",
        "\n",
        "    #Augment test images randomly\n",
        "\n",
        "images_path=\"train\" #path to original images\n",
        "augmented_path=\"train\" # path to store augmented images\n",
        "images=[] # to store paths of images from folder\n",
        "\n",
        "for im in os.listdir(images_path):  # read image name from folder and append its path into \"images\" array     \n",
        "    images.append(os.path.join(images_path,im))\n",
        "\n",
        "images_to_generate=10000 #you can change this value according to your requirement\n",
        "\n",
        "for i in range(images_to_generate):    \n",
        "    image=random.choice(images)\n",
        "    id = image[6:-4]\n",
        "    label = data[data['id'] == id].iloc[0]['label']\n",
        "    data = data.append({\"id\":'augmented_'+id,'label':label},ignore_index=True)\n",
        "    transformed_image= readCroppedImage(image)\n",
        "    new_image_path= \"train/augmented_%s.tif\" %(id)\n",
        "    cv2.imwrite(new_image_path, transformed_image) # save transformed image to path\n",
        "\n",
        "#Save new label file which has the augmented images\n",
        "data.to_csv('new_train_labels.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "e18560bf69d3dfc0c4772e7c79bb119fd2eb634b",
        "id": "unznDo1d4E6L",
        "outputId": "8d2404bc-f5ac-493f-ffed-c5c18617671e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load the new csv that now includes the augmented images \n",
        "df_data = pd.read_csv('../content/new_train_labels.csv')\n",
        "\n",
        "# Check the class distribution\n",
        "df_data['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    137223\n",
              "1     93627\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c1150500d2772b7f36cdaa5aa5fd7f0fb4a72628",
        "id": "OtMFx-K64E6U"
      },
      "source": [
        "#### Balance the target distribution\n",
        "As decided earlier with the variable SAMPLE_SIZE, we will subset our original data into 160000 images half labelled 0, the other labelled 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "270fc18640b552ecc3cb0e1dd3036441db7a4a2b",
        "id": "2-w-l9se4E6U",
        "outputId": "a5f33991-7ce3-47f0-cc05-ff8c44777e19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# take a random sample of class 0 with size equal to num samples in class 1\n",
        "df_0 = df_data[df_data['label'] == 0].sample(SAMPLE_SIZE, random_state = 101)\n",
        "# filter out class 1\n",
        "df_1 = df_data[df_data['label'] == 1].sample(SAMPLE_SIZE, random_state = 101)\n",
        "\n",
        "# concat the dataframes\n",
        "df_data = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n",
        "# shuffle\n",
        "df_data = shuffle(df_data)\n",
        "\n",
        "df_data['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    80000\n",
              "0    80000\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "a166dec3ef84c66ad9cd815b63fc1a753df2eb76",
        "id": "QyBs-M6C4E6X",
        "outputId": "65af6d54-9cb7-4689-9b89-74a1f4f4f020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>44093</th>\n",
              "      <td>199391</td>\n",
              "      <td>81623b9afe6c5f48ceb3ea0819b3880bccbeb628</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125344</th>\n",
              "      <td>189356</td>\n",
              "      <td>eb80ed7ca26a6a72ae2ef08951f15f0a789ada2f</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63003</th>\n",
              "      <td>221637</td>\n",
              "      <td>augmented_2085718fe57cdd057aac28b0f270a2052250...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142531</th>\n",
              "      <td>10013</td>\n",
              "      <td>9c8a15a45c21b51c911d9978ed019381a8c37d6e</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96910</th>\n",
              "      <td>163741</td>\n",
              "      <td>8645c490d084778555ca96d8a7e92acee5987fa4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0                                                 id  label\n",
              "44093       199391           81623b9afe6c5f48ceb3ea0819b3880bccbeb628      0\n",
              "125344      189356           eb80ed7ca26a6a72ae2ef08951f15f0a789ada2f      1\n",
              "63003       221637  augmented_2085718fe57cdd057aac28b0f270a2052250...      0\n",
              "142531       10013           9c8a15a45c21b51c911d9978ed019381a8c37d6e      1\n",
              "96910       163741           8645c490d084778555ca96d8a7e92acee5987fa4      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "15ba9792e6a370b7560330af15b3cfe21185c1cb",
        "id": "Uaw0TI4b4E6Z",
        "outputId": "5d500690-f6fb-4e89-8f5a-8cd85eb8f2eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# train_test_split\n",
        "\n",
        "# stratify=y creates a balanced validation set.\n",
        "y = df_data['label']\n",
        "\n",
        "df_train, df_val = train_test_split(df_data, test_size=0.10, random_state=101, stratify=y)\n",
        "\n",
        "print(df_train.shape)\n",
        "print(df_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(144000, 3)\n",
            "(16000, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "7de70d915a5f1d2599725e00bdb3b9103d947883",
        "id": "_T-uq7nU4E6b",
        "outputId": "91453a5f-a570-4b12-ddec-8eb93607d959",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check the training set counts\n",
        "df_train['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    72000\n",
              "0    72000\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "392c0eea00be8e43a6e55438d1458650e842030b",
        "id": "2Dl_49Ac4E6d",
        "outputId": "ae100b06-8bcb-461a-be59-87b286c37c55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check the validation set counts\n",
        "df_val['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    8000\n",
              "0    8000\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "ba17dd34b75367fc61df6634d51dac94c3ab4951",
        "id": "QwowQqdn4E6f"
      },
      "source": [
        "### Create a Directory Structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "ff8acc2e92a1b1b5002d6e1bf9a1180c3256f19d",
        "id": "VOLN9jPp4E6f"
      },
      "source": [
        "# Create a new directory\n",
        "base_dir = 'base_dir'\n",
        "os.mkdir(base_dir)\n",
        "\n",
        "\n",
        "#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n",
        "\n",
        "# now we create 2 folders inside 'base_dir':\n",
        "\n",
        "# train_dir\n",
        "    # a_no_tumor_tissue\n",
        "    # b_has_tumor_tissue\n",
        "\n",
        "# val_dir\n",
        "    # a_no_tumor_tissue\n",
        "    # b_has_tumor_tissue\n",
        "\n",
        "\n",
        "# create a path to 'base_dir' to which we will join the names of the new folders\n",
        "# train_dir\n",
        "train_dir = os.path.join(base_dir, 'train_dir')\n",
        "os.mkdir(train_dir)\n",
        "\n",
        "# val_dir\n",
        "val_dir = os.path.join(base_dir, 'val_dir')\n",
        "os.mkdir(val_dir)\n",
        "\n",
        "\n",
        "\n",
        "# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n",
        "# Inside each folder we create seperate folders for each class\n",
        "\n",
        "# create new folders inside train_dir\n",
        "no_tumor_tissue = os.path.join(train_dir, 'a_no_tumor_tissue')\n",
        "os.mkdir(no_tumor_tissue)\n",
        "has_tumor_tissue = os.path.join(train_dir, 'b_has_tumor_tissue')\n",
        "os.mkdir(has_tumor_tissue)\n",
        "\n",
        "\n",
        "# create new folders inside val_dir\n",
        "no_tumor_tissue = os.path.join(val_dir, 'a_no_tumor_tissue')\n",
        "os.mkdir(no_tumor_tissue)\n",
        "has_tumor_tissue = os.path.join(val_dir, 'b_has_tumor_tissue')\n",
        "os.mkdir(has_tumor_tissue)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "03ca5d4b8b027c2712d7096314d3a79ef829b23c",
        "id": "ou3Ewe4d4E6h",
        "outputId": "d4d36c35-051e-4091-fb5e-0bae4f10c547",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# check that the folders have been created\n",
        "os.listdir('base_dir/train_dir')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a_no_tumor_tissue', 'b_has_tumor_tissue']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6a2e56340ba18f3b63c1b129fd995fecfadaa21d",
        "id": "XYei61Px4E6j"
      },
      "source": [
        "### Transfer the images into the folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "e84c8a9642b030094b1888af3299063f883112a6",
        "id": "N7J98cj34E6k"
      },
      "source": [
        "# Set the id as the index in df_data\n",
        "df_data.set_index('id', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X33kTZ7Jw5BI",
        "outputId": "8569d1fd-f957-4bbc-fafe-4f86af499fdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for image in train_list:\n",
        "    \n",
        "    # the id in the csv file does not have the .tif extension therefore we add it here\n",
        "    fname = image + '.tif'\n",
        "    # get the label for a certain image\n",
        "    target = df_data.loc[image,'label']\n",
        "    \n",
        "    print(int(target))\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qtc9gSQsyFjA",
        "outputId": "9780391b-6015-44b6-a59d-cda32e82aaac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_list[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3921341dcabd03dc16dfd373e8821e490a2479bd'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG_nlgrXy3Id",
        "outputId": "0eab37b0-9ff7-4a0e-f30d-60fc4e50005f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "df_data.head(n=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>81623b9afe6c5f48ceb3ea0819b3880bccbeb628</th>\n",
              "      <td>199391</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          Unnamed: 0  label\n",
              "id                                                         \n",
              "81623b9afe6c5f48ceb3ea0819b3880bccbeb628      199391      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "afb8969a9ee75c13bddc808a4bcc326611baaaaf",
        "id": "YnNDcfjb4E6l"
      },
      "source": [
        "# Get a list of train and val images\n",
        "train_list = list(df_train['id'])\n",
        "val_list = list(df_val['id'])\n",
        "\n",
        "\n",
        "# Transfer the train images\n",
        "count = 0\n",
        "for image in train_list:\n",
        "    count += 1\n",
        "    # the id in the csv file does not have the .tif extension therefore we add it here\n",
        "    fname = image + '.tif'\n",
        "    # get the label for a certain image\n",
        "    target = df_data.loc[image,'label']\n",
        "\n",
        "    # these must match the folder names\n",
        "    if target.any() == 0:\n",
        "        label = 'a_no_tumor_tissue'\n",
        "    if target.any() == 1:\n",
        "        label = 'b_has_tumor_tissue'\n",
        "    \n",
        "    # source path to image\n",
        "    src = os.path.join('../content/train', fname)\n",
        "    # destination path to image\n",
        "    dst = os.path.join(train_dir, label, fname)\n",
        "    # copy the image from the source to the destination\n",
        "    shutil.copyfile(src, dst)\n",
        "\n",
        "\n",
        "# Transfer the val images\n",
        "\n",
        "for image in val_list:\n",
        "    \n",
        "    # the id in the csv file does not have the .tif extension therefore we add it here\n",
        "    fname = image + '.tif'\n",
        "    # get the label for a certain image\n",
        "    target = df_data.loc[image,'label']\n",
        "    \n",
        "    # these must match the folder names\n",
        "    if target.any() == 0:\n",
        "        label = 'a_no_tumor_tissue'\n",
        "    if target.any() == 1:\n",
        "        label = 'b_has_tumor_tissue'\n",
        "    \n",
        "\n",
        "    # source path to image\n",
        "    src = os.path.join('../content/train', fname)\n",
        "    # destination path to image\n",
        "    dst = os.path.join(val_dir, label, fname)\n",
        "    # copy the image from the source to the destination\n",
        "    shutil.copyfile(src, dst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "71532bfc32608289b1f773ffdbc8a7cea1bfb94c",
        "id": "okgn0GOz4E6n"
      },
      "source": [
        "# check how many train images we have in each folder\n",
        "print(len(os.listdir('base_dir/train_dir/a_no_tumor_tissue')))\n",
        "print(len(os.listdir('base_dir/train_dir/b_has_tumor_tissue')))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "897e9df543bb65b47bb00019dc681125ca08ee5d",
        "id": "hwbQI2li4E6r"
      },
      "source": [
        "# check how many val images we have in each folder\n",
        "print(len(os.listdir('base_dir/val_dir/a_no_tumor_tissue')))\n",
        "print(len(os.listdir('base_dir/val_dir/b_has_tumor_tissue')))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f8dce940ee8a7a42aacb062e4c6b5a4a54dba58f",
        "id": "9Ea8AeIz4E6u"
      },
      "source": [
        "### Set Up the Generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "ef4fe7be09f11ff4badfd22d5fd5e03f8521ed58",
        "id": "spfnN1L64E6v"
      },
      "source": [
        "train_path = 'base_dir/train_dir'\n",
        "valid_path = 'base_dir/val_dir'\n",
        "\n",
        "num_train_samples = len(df_train)\n",
        "num_val_samples = len(df_val)\n",
        "train_batch_size = 10\n",
        "val_batch_size = 10\n",
        "\n",
        "\n",
        "train_steps = np.ceil(num_train_samples / train_batch_size)\n",
        "val_steps = np.ceil(num_val_samples / val_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "68fbd9d5fbb80859a82f94a12e335ce05a93bd51",
        "id": "7TmUfp1S4E6w",
        "outputId": "4d05f265-ef3e-4221-9f60-569e77136c47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "train_gen = datagen.flow_from_directory(train_path,\n",
        "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "                                        batch_size=train_batch_size,\n",
        "                                        class_mode='categorical')\n",
        "\n",
        "val_gen = datagen.flow_from_directory(valid_path,\n",
        "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "                                        batch_size=val_batch_size,\n",
        "                                        class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 144000 images belonging to 2 classes.\n",
            "Found 16000 images belonging to 2 classes.\n",
            "Found 16000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SV62R0uBehJ"
      },
      "source": [
        "# 3. Model creation assessment\n",
        "\n",
        "This is the stage where we create a model that trains on the data.\n",
        "In our case, we chose to manually create a neural network model implemented with the keras library. We created a feedforward neural network with 24 layers all using the relu activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "b9835ea0fd0bca54138904895c39d38227a70c22",
        "id": "ekJRK1m94E60",
        "outputId": "991bc2d6-04ad-4c6b-d2fa-5fd1e6eff22f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Model architecture\n",
        "kernel_size = (3,3)\n",
        "pool_size= (2,2)\n",
        "first_filters = 32\n",
        "second_filters = 64\n",
        "third_filters = 128\n",
        "\n",
        "dropout_conv = 0.3\n",
        "dropout_dense = 0.3\n",
        "\n",
        "# Neural network creation and layer adding\n",
        "model = Sequential()\n",
        "model.add(Conv2D(first_filters, kernel_size, activation = 'relu', input_shape = (96, 96, 3)))\n",
        "model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
        "model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = pool_size)) \n",
        "model.add(Dropout(dropout_conv))\n",
        "\n",
        "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
        "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
        "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
        "model.add(MaxPooling2D(pool_size = pool_size))\n",
        "model.add(Dropout(dropout_conv))\n",
        "\n",
        "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
        "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
        "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
        "model.add(MaxPooling2D(pool_size = pool_size))\n",
        "model.add(Dropout(dropout_conv))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation = \"relu\"))\n",
        "model.add(Dropout(dropout_dense))\n",
        "model.add(Dense(2, activation = \"softmax\"))\n",
        "\n",
        "# Key details for each layer\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 94, 94, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 92, 92, 32)        9248      \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 90, 90, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 45, 45, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 45, 45, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 43, 43, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 41, 41, 64)        36928     \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 39, 39, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 19, 19, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 19, 19, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 17, 17, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 15, 15, 128)       147584    \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 13, 13, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               1179904   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 514       \n",
            "=================================================================\n",
            "Total params: 1,661,186\n",
            "Trainable params: 1,661,186\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "75cfc4fcb8dd3408d1c4fcf8cd85e0e2f5b611d7",
        "id": "VsvJFuO-4E62"
      },
      "source": [
        "### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "9de9715f49a63b55775b10abd2f461b395e23b5d",
        "id": "ZKlpjCv64E62"
      },
      "source": [
        "# Compile the model\n",
        "model.compile(Adam(lr=0.0001), loss='binary_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "227d84a4f44c0b7256855c06ba04dabd58d89d84",
        "id": "0AICE1jf4E63",
        "outputId": "ce5db15b-d7d0-47b3-b2c3-4be5e5836980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Get the labels that are associated with each index\n",
        "print(val_gen.class_indices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a_no_tumor_tissue': 0, 'b_has_tumor_tissue': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "a746769db61563f226288eba9aa8a6584b9e8e0b",
        "id": "GCWNn3Sm4E65",
        "outputId": "e3672b98-3a9a-47c2-9b52-25a31ec4ff25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "# Train through 20 epochs in hopes to improve accuracy\n",
        "filepath = \"model.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, \n",
        "                             save_best_only=True, mode='max')\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=2, \n",
        "                                   verbose=0, mode='max', min_lr=0.00001)\n",
        "                                                      \n",
        "callbacks_list = [checkpoint, reduce_lr]\n",
        "\n",
        "history = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n",
        "                    validation_data=val_gen,\n",
        "                    validation_steps=val_steps,\n",
        "                    epochs=20, verbose=0,\n",
        "                   callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.91037, saving model to model.h5\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.91037\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.91037 to 0.91219, saving model to model.h5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.91219 to 0.92200, saving model to model.h5\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.92200 to 0.92625, saving model to model.h5\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.92625 to 0.92706, saving model to model.h5\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.92706 to 0.92813, saving model to model.h5\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.92813\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.92813 to 0.93831, saving model to model.h5\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.93831\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.93831\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.93831\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.93831 to 0.94275, saving model to model.h5\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.94275\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.94275\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.94275 to 0.94356, saving model to model.h5\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.94356\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.94356 to 0.94463, saving model to model.h5\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.94463\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.94463\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kprz3azBf8S_",
        "outputId": "1eb776cd-cec6-4470-eeac-2be3c7df57dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "\n",
        "# VGG model without the last classifier layers (include_top = False)\n",
        "vgg16_model = VGG16(include_top = False,\n",
        "                    input_shape = (96,96,3),\n",
        "                    #weights='../input/VGG16weights/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
        "                    weights = 'imagenet')\n",
        "    \n",
        "# Freeze the layers \n",
        "for layer in vgg16_model.layers[:-12]:\n",
        "    layer.trainable = False\n",
        "    \n",
        "# Check the trainable status of the individual layers\n",
        "for layer in vgg16_model.layers:\n",
        "    print(layer, layer.trainable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f8912ae2cc0> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f872cedfe80> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f889e42f2b0> False\n",
            "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f889e42ff60> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f889e10f898> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f872d0036a0> False\n",
            "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f889e1109e8> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f889e0dca58> True\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f872d0fab38> True\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f889e10ccf8> True\n",
            "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f889e138fd0> True\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f889e138400> True\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f889e10cda0> True\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f889e4192b0> True\n",
            "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f889e40c5c0> True\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f889e419978> True\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f889e4195c0> True\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f889e41d828> True\n",
            "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f889e417b38> True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I20HlgwQgIlw"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Flatten,Dropout\n",
        "from keras import optimizers\n",
        "\n",
        "model = Sequential()\n",
        "model.add(vgg16_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation=\"softmax\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjGpGyKHgO0z"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer=optimizers.SGD(lr=0.00001, momentum=0.95),metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryqMwLKceiEu",
        "outputId": "b179ece4-c1d9-47e2-fe90-4626058baef3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        }
      },
      "source": [
        "### Evaluate the model using the val set\n",
        "\n",
        "filepath_2 = \"model.h5_2\"\n",
        "checkpoint_2 = ModelCheckpoint(filepath_2, monitor='val_accuracy', verbose=1, \n",
        "                             save_best_only=True, mode='max')\n",
        "\n",
        "reduce_lr_2 = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=2, \n",
        "                                   verbose=0, mode='max', min_lr=0.00001)\n",
        "                              \n",
        "                              \n",
        "callbacks_list_2 = [checkpoint_2, reduce_lr_2]\n",
        "\n",
        "history_2 =  model.fit_generator(train_gen, steps_per_epoch=train_steps, \n",
        "                    validation_data=val_gen,\n",
        "                    validation_steps=val_steps,\n",
        "                    epochs=20, verbose=0,\n",
        "                   callbacks=callbacks_list_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89844, saving model to model.h5_2\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: model.h5_2/assets\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.89844 to 0.92106, saving model to model.h5_2\n",
            "INFO:tensorflow:Assets written to: model.h5_2/assets\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.92106\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.92106 to 0.93094, saving model to model.h5_2\n",
            "INFO:tensorflow:Assets written to: model.h5_2/assets\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.93094 to 0.93831, saving model to model.h5_2\n",
            "INFO:tensorflow:Assets written to: model.h5_2/assets\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.93831\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.93831\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.93831 to 0.94675, saving model to model.h5_2\n",
            "INFO:tensorflow:Assets written to: model.h5_2/assets\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.94675 to 0.94850, saving model to model.h5_2\n",
            "INFO:tensorflow:Assets written to: model.h5_2/assets\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.94850\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.94850 to 0.94981, saving model to model.h5_2\n",
            "INFO:tensorflow:Assets written to: model.h5_2/assets\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.94981 to 0.95300, saving model to model.h5_2\n",
            "INFO:tensorflow:Assets written to: model.h5_2/assets\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.95300 to 0.95369, saving model to model.h5_2\n",
            "INFO:tensorflow:Assets written to: model.h5_2/assets\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.95369 to 0.95394, saving model to model.h5_2\n",
            "INFO:tensorflow:Assets written to: model.h5_2/assets\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.95394\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.95394\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.95394\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.95394\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.95394\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.95394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuan38W8z30n"
      },
      "source": [
        "model.save(\"vgg16_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN81JLP0z7ou"
      },
      "source": [
        "#### Save the History Logs for both models as CSV files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF01vZEP0DcG"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# convert the history.history dict to a pandas DataFrame:     \n",
        "hist_df_keras = pd.DataFrame(history.history) \n",
        "hist_df_vgg16 = pd.DataFrame(history_2.history) \n",
        "# or save to csv: \n",
        "hist_csv_file = 'history_keras_model.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df_keras.to_csv(f)\n",
        "\n",
        "hist_csv_file_16 = 'history_vgg16_model.csv'\n",
        "with open(hist_csv_file_16, mode='w') as f:\n",
        "    hist_df_vgg16.to_csv(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk8HiTNbDERo"
      },
      "source": [
        "# 4. Presentation of findings\n",
        "\n",
        "This is the section where we analyze the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PVT9V3uDf2s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}